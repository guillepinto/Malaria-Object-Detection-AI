{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone our repo to get the no trainer script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your GitHub token:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Malaria-Object-Detection-AI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Malaria-Object-Detection-AI\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Clone the private repository in Colab\n",
    "token = getpass('Enter your GitHub token: ')\n",
    "repo_url = f\"https://{token}:x-oauth-basic@github.com/guillepinto/Malaria-Object-Detection-AI.git\"\n",
    "\n",
    "# Switch to the necessary branch\n",
    "branch = \"main\"\n",
    "\n",
    "# Command to clone the repository\n",
    "os.system(f\"git clone -b {branch} {repo_url}\")\n",
    "\n",
    "# Access to the repo\n",
    "%cd Malaria-Object-Detection-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! make requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# log in hugging face hug to load data and models\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# log in wand to track the experiments\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that accelerate works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/root/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from accelerate import notebook_launcher\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "# Configure accelerate\n",
    "write_basic_config(mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch /opt/conda/lib/python3.10/site-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: NO\n",
      "stdout: Num processes: 1\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test split between processes evenly**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a datasets.Dataset**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: 0 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "stdout:        device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "stdout: \n",
      "stdout: **Test reinstantiated state**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "! accelerate test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your sh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Malaria-Object-Detection-AI'\n",
      "/kaggle/working/Malaria-Object-Detection-AI\n",
      "facebook-detr-resnet-50-finetuned-malaria_1x8-4-600.sh\tscript.sh\n",
      "hustvl-yolos-tiny-finetuned-malaria_1x8-6-600.sh\n"
     ]
    }
   ],
   "source": [
    "%cd Malaria-Object-Detection-AI\n",
    "\n",
    "# Define the script content\n",
    "script_content = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# ConfiguraciÃ³n de variables\n",
    "MODEL_NAME=\"microsoft/conditional-detr-resnet-50\"\n",
    "DATASET_NAME=\"SemilleroCV/lacuna_malaria\"\n",
    "OUTPUT_DIR=\"conditional-detr\"\n",
    "NUM_TRAIN_EPOCHS=1\n",
    "CHECKPOINTING_STEPS=1000\n",
    "IMAGE_SQUARE_SIZE=800\n",
    "TRAIN_BATCH_SIZE=16\n",
    "EVAL_BATCH_SIZE=16\n",
    "LEARNING_RATE=2e-4\n",
    "GRADIENT_ACCUMULATION_STEPS=1\n",
    "ADAM_BETA1=0.9\n",
    "ADAM_BETA2=0.999\n",
    "ADAM_EPSILON=1e-8\n",
    "LR_SCHEDULER_TYPE=\"linear\"\n",
    "NUM_WARMUP_STEPS=0\n",
    "IGNORE_MISMATCHED_SIZES=\"--ignore_mismatched_sizes\"\n",
    "WITH_TRACKING=\"--with_tracking\"\n",
    "PUSH_TO_HUB=\"--push_to_hub\"\n",
    "REPORT_TO=\"wandb\"\n",
    "WANDB_PROJECT=\"challenge-malaria\"\n",
    "SEED=3407\n",
    "\n",
    "# CHECKPOINT=\"Microsoft-conditional-detr/step_2000\"\n",
    "\n",
    "EFFECTIVE_BATCH_SIZE=$((TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS))\n",
    "MODEL_ID=\"SemilleroCV/${OUTPUT_DIR}-${MAX_TRAIN_STEPS}steps\"\n",
    "\n",
    "# GeneraciÃ³n del nuevo nombre del archivo basado en la configuraciÃ³n\n",
    "NEW_SCRIPT_NAME=\"${OUTPUT_DIR}_1x${EFFECTIVE_BATCH_SIZE}-${MAX_TRAIN_STEPS}-${IMAGE_SQUARE_SIZE}.sh\"\n",
    "\n",
    "# Llamada al script de entrenamiento\n",
    "accelerate launch run_object_detection_no_trainer.py \\\\\n",
    "    --model_name_or_path \"$MODEL_NAME\" \\\\\n",
    "    --dataset_name \"$DATASET_NAME\" \\\\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\\\n",
    "    --num_train_epochs $NUM_TRAIN_EPOCHS \\\\\n",
    "    --image_square_size $IMAGE_SQUARE_SIZE \\\\\n",
    "    --per_device_train_batch_size $TRAIN_BATCH_SIZE \\\\\n",
    "    --per_device_eval_batch_size $EVAL_BATCH_SIZE \\\\\n",
    "    --checkpointing_steps $CHECKPOINTING_STEPS \\\\\n",
    "    --learning_rate $LEARNING_RATE \\\\\n",
    "    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \\\\\n",
    "    --adam_beta1 $ADAM_BETA1 \\\\\n",
    "    --adam_beta2 $ADAM_BETA2 \\\\\n",
    "    --adam_epsilon $ADAM_EPSILON \\\\\n",
    "    --lr_scheduler_type $LR_SCHEDULER_TYPE \\\\\n",
    "    --num_warmup_steps $NUM_WARMUP_STEPS \\\\\n",
    "    $IGNORE_MISMATCHED_SIZES \\\\\n",
    "    $WITH_TRACKING \\\\\n",
    "    --report_to $REPORT_TO \\\\\n",
    "    --wandb_project \"$WANDB_PROJECT\" \\\\\n",
    "    --seed $SEED \\\\\n",
    "    #--resume_from_checkpoint \"$CHECKPOINT\" \\\\\n",
    "    #$PUSH_TO_HUB \\\\\n",
    "    #--hub_model_id \"$MODEL_ID\" \\\\\n",
    "\n",
    "# Renombrar el archivo script\n",
    "mv \"$0\" \"scripts/$NEW_SCRIPT_NAME\"\n",
    "\"\"\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "!mkdir -p scripts\n",
    "\n",
    "# Write the script to a file inside the scripts folder\n",
    "with open('scripts/script.sh', 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "# Make the script executable\n",
    "!chmod +x scripts/script.sh\n",
    "\n",
    "# List files to confirm it's created\n",
    "!ls scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301/301 [00:00<00:00, 2.08MB/s]\n",
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--conditional-detr-resnet-50/snapshots/8f8795fb7c319c7862d4f4cd699e76bb09cf2593/preprocessor_config.json\n",
      "Image processor ConditionalDetrImageProcessor {\n",
      "  \"do_convert_annotations\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"format\": \"coco_detection\",\n",
      "  \"image_mean\": [\n",
      "    0.629,\n",
      "    0.544,\n",
      "    0.597\n",
      "  ],\n",
      "  \"image_processor_type\": \"ConditionalDetrImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.254,\n",
      "    0.226,\n",
      "    0.241\n",
      "  ],\n",
      "  \"pad_size\": {\n",
      "    \"height\": 800,\n",
      "    \"width\": 800\n",
      "  },\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"max_height\": 800,\n",
      "    \"max_width\": 800\n",
      "  }\n",
      "}\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguillepinto\u001b[0m (\u001b[33mchallenge-malaria\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/Malaria-Object-Detection-AI/wandb/run-20241206_024742-rs9n9sgj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-fire-163\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/challenge-malaria/challenge-malaria\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/challenge-malaria/challenge-malaria/runs/rs9n9sgj\u001b[0m\n",
      "  0%|                                                   | 0/155 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 69/155 [03:45<03:39,  2.55s/it]/opt/conda/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:478: RuntimeWarning: invalid value encountered in divide\n",
      "  & (clipped_box_areas / denormalized_box_areas >= min_visibility - epsilon)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [08:28<00:00,  1.55s/it]\n",
      "  0%|                                                    | 0/18 [00:00<?, ?it/s]\u001b[AThe `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "\n",
      "  6%|â–ˆâ–ˆâ–                                         | 1/18 [00:08<02:29,  8.77s/it]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 2/18 [00:09<01:05,  4.08s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 3/18 [00:10<00:40,  2.67s/it]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 4/18 [00:15<00:51,  3.71s/it]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 5/18 [00:16<00:34,  2.69s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 6/18 [00:17<00:24,  2.05s/it]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 7/18 [00:18<00:18,  1.69s/it]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 8/18 [00:23<00:27,  2.80s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 9/18 [00:24<00:19,  2.16s/it]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 10/18 [00:25<00:15,  1.88s/it]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 11/18 [00:26<00:10,  1.53s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 12/18 [00:31<00:16,  2.69s/it]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/18 [00:32<00:10,  2.08s/it]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 14/18 [00:33<00:06,  1.68s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 15/18 [00:33<00:04,  1.37s/it]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/18 [00:36<00:03,  1.65s/it]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17/18 [00:36<00:01,  1.35s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.06s/it]\u001b[A\n",
      "Best metric found: 0.0897. Saving model...\n",
      "\n",
      "  0%|                                                    | 0/18 [00:00<?, ?it/s]\u001b[AThe `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "\n",
      "  6%|â–ˆâ–ˆâ–                                         | 1/18 [00:08<02:22,  8.41s/it]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                       | 2/18 [00:09<01:02,  3.90s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 3/18 [00:10<00:37,  2.52s/it]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 4/18 [00:17<01:00,  4.36s/it]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 5/18 [00:17<00:39,  3.06s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                             | 6/18 [00:18<00:27,  2.26s/it]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 7/18 [00:19<00:20,  1.83s/it]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 8/18 [00:24<00:27,  2.79s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 9/18 [00:25<00:19,  2.15s/it]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 10/18 [00:26<00:14,  1.85s/it]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 11/18 [00:27<00:10,  1.51s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 12/18 [00:32<00:15,  2.63s/it]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/18 [00:32<00:10,  2.03s/it]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 14/18 [00:33<00:06,  1.64s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 15/18 [00:34<00:04,  1.35s/it]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 16/18 [00:36<00:03,  1.72s/it]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 17/18 [00:37<00:01,  1.40s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.10s/it]\u001b[A\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 map â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              map_50 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              map_75 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     map_Trophozoite â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             map_WBC â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           map_large â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          map_medium â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           map_small â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               mar_1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              mar_10 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             mar_100 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: mar_100_Trophozoite â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         mar_100_WBC â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mar_large â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          mar_medium â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mar_small â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 map 0.0287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              map_50 0.0897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              map_75 0.0074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     map_Trophozoite 0.0136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             map_WBC 0.0438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           map_large 0.0368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          map_medium 0.028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           map_small 0.0125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               mar_1 0.0266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              mar_10 0.1572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             mar_100 0.2502\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: mar_100_Trophozoite 0.2159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         mar_100_WBC 0.2845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mar_large 0.4103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          mar_medium 0.2542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mar_small 0.1397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step 155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.43943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.39254\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mgenial-fire-163\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/challenge-malaria/challenge-malaria/runs/rs9n9sgj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/challenge-malaria/challenge-malaria\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241206_024742-rs9n9sgj/logs\u001b[0m\n",
      "Configuration saved in conditional-detr/config.json\n",
      "Model weights saved in conditional-detr/model.safetensors\n",
      "Image processor saved in conditional-detr/preprocessor_config.json\n",
      "/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2380: UserWarning: Run (rs9n9sgj) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [10:01<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "! sh /kaggle/working/Malaria-Object-Detection-AI/scripts/script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "preprocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 2.56MB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.41k/1.41k [00:00<00:00, 8.32MB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174M/174M [00:08<00:00, 20.0MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1178/1178 [05:52<00:00,  3.34it/s]\n",
      "Archivo de submission guardado como submissions/pimientoyolo_microsoft-conditional-detr-resnet-50/pimientoyolo_microsoft-conditional-detr-resnet-50_submission_1206_0317.csv\n"
     ]
    }
   ],
   "source": [
    "# the script receives a path or the id of the model in hugging face and the path of the test images\n",
    "# in this case we load the best model we got from Transformers\n",
    "! python /kaggle/working/Malaria-Object-Detection-AI/make_submission.py \\\n",
    "    --model_name pimientoyolo/microsoft-conditional-detr-resnet-50 \\\n",
    "    --test_path /kaggle/working/test/images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as inference.png\n"
     ]
    }
   ],
   "source": [
    "# this script receives as first parameter the path to a submission file and as second the path to the images,\n",
    "# it can also receive as 3 additional parameters the paths to 3 specific images or by default it will make\n",
    "# inference on random images.\n",
    "! python /kaggle/working/Malaria-Object-Detection-AI/make_inference.py /kaggle/working/Malaria-Object-Detection-AI/submissions/yolo11_exp9_submission.csv /kaggle/working/test/images"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
